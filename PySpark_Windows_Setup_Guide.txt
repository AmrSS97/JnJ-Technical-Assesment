-- PySpark Installation & Configuration Guide --
-- This is a step-by-step guidance file to install & configure PySpark correctly to avoid any technical issues --

PySpark requires Python and Java. Installing PySpark via pip includes Apache Spark and is sufficient for local development and Databricks-style workflows. Hadoop installation is not required unless working with secured or on-prem HDFS clusters.

1. Python Installation

- In order to use PySpark, we need to install Python & pip on our local machine. Just simply google how to download Python or use this link: https://www.python.org/downloads/ and download the Python install manager or the standalone installer & follow the steps then make a quick check by running "python --version" command in the powershell/cmd or terminal in case of linux and the output should be something like "Python 3.X.X". You should check that you want pip to be installed as well in the installer & then validate by running the command "pip --version"

2. PySpark Installation

- Next we use pip to install PySpark interface by running the command "pip install pyspark"

3. Java & Hadoop

- We need to install Java, preferably Java 8 as it is the most compatible with Spark, and same as Python just google Java download or use the link: https://www.oracle.com/java/technologies/downloads/#java8 to download the JDK then just install it and follow the steps then simply run "Java -version" on the powershell/cmd and if your installation went smoothly there should be no errors and some info about Java version you installed should appear as an output.

(Optional As Mentioned Above)
- Hadoop bineries need to be configured as well, however in that case we will rely this time on a widely used trusted Github mirror for Hadoop Windows binaries as Hadoop community never officially packaged Windows binaries so most people rely on this repo: https://github.com/steveloughran/winutils This repo is maintained by a Hadoop committer and is considered the de-facto standard.

4. Directories & Environment Variables

- Create a simple directory in your C drive, called hadoop for example then create a folder inside it called "bin" then after downloading the repository we mentioned zip file and extracting it, make sure to open the folder that marks the same version compatible with your PySpark then copy all the files and paste it in the bin folder you created earlier

- Spark needs to locate Java at runtime therefore we need to create an environment variable so that Spark can be able to do that. Simply open your powershell/cmd and run the below commands:
     (Recommended)
     - setx JAVA_HOME path_to_your_JDK_between_double_quotations
     - setx PATH "%PATH%;%JAVA_HOME%\bin"
	 (Optional)
     - setx HADOOP_HOME path_to_your_hadoop_directory_in_C_drive
     - setx PATH "%PATH%;path_to_bin_folder_in_hadoop_folder"
	   
- By running the previous commands, environment variables for the dependencies were created for Spark to use.

5. Validation & Testing

- To make sure everything was configured correctly, open a new powershell/cmd and run the command "spark-submit --version" it should output a welcoming window with the name Spark, ready to execute Pythone scripts that uses PySpark interface and related libraries.

Note: again this is for Windows OS not Linux although the steps are similar yet there are differences.

