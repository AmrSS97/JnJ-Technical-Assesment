-- Task 2 Steps --
-- Please try to solve the task first without using this file as a guidance --

First Approach:

- Start by creating an account on Databricks platform

- Your SQL script should be similar to the one found in SQL folder in the repository, the one called databricks_analysis.sql

- If you want to try the script mentioned earlier, make sure to create a new schema in the Catalog's workspace called j&j and import the CSV files to be createdas new tables.

- Create a new Notebook in your main Workspace, feel free to create any sub folders before it, and make sure to choose SQL as your language to be executed then copy and paste the SQL query in the code cell of the Notebook.

- Run the code cell and you should see the resulting table.
Optional: you can create a Databricks job to schedule this task to run either manually or as per a certain schedule.

Second Approach:

- In order to see the results of your SQL query without the need to install a database server and import the three CSV files as three tables in a schema inside it, we can use Spark SQL.

- In order to implement that, you should create two files similar to analysis.sql, the one found in SQL folder and run_spark_sql.py, the one found in Spark folder in our repository.

- analysis.sql script is pretty much the same as databricks_analysis.sql with minor differences, and you can view the python code in the run_spark_sql.py which is simple to understand and well-documented. However, we are always open to questions.

- Simply after having the two files on your local machine, if you are on Windows, open powershell/cmd and simply run the command "spark-submit" then the name of your Python script then the paths to your csv data file and SQL script respectively. You should see the resulting table in a few milliseconds.

Note: please make sure to correctly configure PySpark which needs dependencies like Java, Spark, Python, and Hadoop, and for any support, please check the PySpark-Setup.txt file.

Thank You For You Patience & Welcome Aboard !!

